apply_library_patches: true
base_image: null
build:
  arguments: {}
  model_server: TrussServer
bundled_packages_dir: packages
data_dir: data
description: Generate text from a instructional prompt with this tuned version of
  LLaMA 7B.
environment_variables: {}
examples_filename: examples.yaml
external_data:
- backend: http_public
  local_data_path: pytorch_model-00001-of-00002.bin
  url: https://baseten-public.s3.us-west-2.amazonaws.com/models/alpaca/pytorch_model-00001-of-00002.bin
- backend: http_public
  local_data_path: pytorch_model-00002-of-00002.bin
  url: https://baseten-public.s3.us-west-2.amazonaws.com/models/alpaca/pytorch_model-00002-of-00002.bin
external_package_dirs: []
hf_cache: null
input_type: Any
live_reload: false
model_class_filename: model.py
model_class_name: Model
model_framework: custom
model_metadata:
  avatar_url: https://cdn.baseten.co/production/static/explore/meta.png
  cover_image_url: https://cdn.baseten.co/production/static/explore/alpaca.png
  example_model_input:
    num_beams: 4
    prompt: What's the meaning of life?
    temperature: 0.1
    top_p: 0.75
  tags:
  - text-generation
model_module_dir: model
model_name: Alpaca 7B
model_type: custom
python_version: py38
requirements:
- torch==2.0.1
- peft==0.3.0
- sentencepiece==0.1.99
- git+https://github.com/huggingface/transformers.git
resources:
  accelerator: A10G
  cpu: '3'
  memory: 14Gi
  use_gpu: true
runtime:
  predict_concurrency: 1
secrets: {}
spec_version: '2.0'
system_packages: []
train:
  resources:
    accelerator: null
    cpu: 500m
    memory: 512Mi
    use_gpu: false
  training_class_filename: train.py
  training_class_name: Train
  training_module_dir: train
  variables: {}
