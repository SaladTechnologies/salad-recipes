{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77588/1031519628.py:207: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  pl.count().alias(\"sample_count\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_total_throughput': 16150.352500000003, 'optimal_vu_count': 193.0, 'input_throughput': 13654.386666666667, 'output_throughput': 2495.9658333333327, 'sample_count': 20}\n",
      "{'input_price_per_token': 1.363139074264052e-08, 'output_price_per_token': 5.452556297056208e-08, 'input_price_per_million': 0.01363139074264052, 'output_price_per_million': 0.05452556297056208, 'input_cost_percentage': 57.76394896689335, 'output_cost_percentage': 42.23605103310664, 'input_tokens_per_second': 13654.386666666667, 'output_tokens_per_second': 2495.9658333333327, 'total_tokens_per_second': 16150.3525}\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import os\n",
    "\n",
    "price_file = \"../../../benchmark/prices.json\"\n",
    "with open(price_file) as f:\n",
    "    prices = json.load(f)\n",
    "\n",
    "vcpu_price = 0.004\n",
    "mem_gb_price = 0.001\n",
    "\n",
    "\n",
    "def get_price_map():\n",
    "    price_map = {}\n",
    "    for gpu_obj in prices[\"items\"]:\n",
    "        gpu_name = gpu_obj[\"name\"].lower()\n",
    "        price_map[gpu_name] = {}\n",
    "        for price_obj in gpu_obj[\"prices\"]:\n",
    "            price_map[gpu_name][price_obj[\"priority\"]\n",
    "                                ] = float(price_obj[\"price\"])\n",
    "    return price_map\n",
    "\n",
    "\n",
    "price_map = get_price_map()\n",
    "gpt4o_mini_price_per_million = {\n",
    "    \"input\": .15,\n",
    "    \"output\": .6\n",
    "}\n",
    "gpt4o_mini_token_multiplier = 2.0\n",
    "\n",
    "\n",
    "def get_df(gpu):\n",
    "    datafile = f\"{gpu}.jsonl\"\n",
    "    node_counts = f\"{gpu}-node-count.csv\"\n",
    "    test_config = f\"{gpu}-test-config.json\"\n",
    "\n",
    "    df_file = f\"{gpu}-df.csv\"\n",
    "\n",
    "    with open(test_config) as f:\n",
    "        test_config = json.load(f)\n",
    "\n",
    "    gpu_uuid = test_config[\"container\"][\"resources\"][\"gpu_classes\"][0]\n",
    "    gpu_obj = next((x for x in prices[\"items\"] if x[\"id\"] == gpu_uuid), None)\n",
    "    gpu_name = gpu_obj[\"name\"].lower()\n",
    "\n",
    "    cost = {}\n",
    "    for priority in price_map[gpu_name]:\n",
    "        cost[priority] = price_map[gpu_name][priority] + (vcpu_price * test_config[\"container\"][\"resources\"][\"cpu\"]) + (\n",
    "            mem_gb_price * test_config[\"container\"][\"resources\"][\"memory\"] // 1024)\n",
    "\n",
    "    if os.path.exists(df_file):\n",
    "        return pl.read_csv(df_file), cost\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    with open(datafile) as f:\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "            if \"data\" in data and \"time\" in data[\"data\"]:\n",
    "                data[\"data\"][\"time\"] = parser.isoparse(data[\"data\"][\"time\"])\n",
    "                all_results.append(data)\n",
    "\n",
    "    tz = all_results[0][\"data\"][\"time\"].tzinfo\n",
    "\n",
    "    with open(node_counts) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            time, count = row\n",
    "            if not time or not count:\n",
    "                continue\n",
    "            all_results.append({\n",
    "                \"type\": \"Point\",\n",
    "                \"metric\": \"node_count\",\n",
    "                \"data\": {\n",
    "                    \"time\": datetime.fromtimestamp(int(time), tz=tz),\n",
    "                    \"value\": int(count)\n",
    "                }\n",
    "            })\n",
    "\n",
    "    metrics = [\"http_req_duration\", \"http_req_failed\",\n",
    "               \"vus\", \"node_count\", \"inputTokens\", \"outputTokens\"]\n",
    "\n",
    "    all_results = sorted(all_results, key=lambda x: x[\"data\"][\"time\"])\n",
    "\n",
    "    first_time = all_results[0][\"data\"][\"time\"]\n",
    "    all_results = [x for x in all_results if x[\"type\"]\n",
    "                   == \"Point\" and x[\"metric\"] in metrics]\n",
    "    results = []\n",
    "    for result in all_results:\n",
    "        time_from_start = (result[\"data\"][\"time\"] - first_time).total_seconds()\n",
    "        value = result[\"data\"][\"value\"]\n",
    "        metric = result[\"metric\"]\n",
    "        results.append({\n",
    "            \"time_from_start\": time_from_start,\n",
    "            \"value\": value,\n",
    "            \"metric\": metric,\n",
    "            \"gpu\": gpu_name,\n",
    "            \"cpu\": test_config[\"container\"][\"resources\"][\"cpu\"],\n",
    "            \"memory\": test_config[\"container\"][\"resources\"][\"memory\"],\n",
    "        })\n",
    "\n",
    "    df = pl.DataFrame(results)\n",
    "    \n",
    "    \n",
    "    df.sort(\"time_from_start\", multithreaded=True)\n",
    "    df.write_csv(df_file)\n",
    "    return df, cost\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "def tokens_per_second_by_vu(df, window_size=5.0, min_vu=30):\n",
    "    \"\"\"\n",
    "    Calculate rolling average input and output tokens per second grouped by number of VUs,\n",
    "    with times rounded to the nearest second and filtering out VU counts below a minimum.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pl.DataFrame): Polars dataframe with schema containing time_from_start, value, metric\n",
    "    window_size (float): Size of the rolling window in seconds (default: 5.0)\n",
    "    min_vu (int): Minimum VU count to include in results (default: 30)\n",
    "    \n",
    "    Returns:\n",
    "    pl.DataFrame: Dataframe with rolling token throughput statistics by VU count\n",
    "    \"\"\"\n",
    "    # Round times to the nearest second \n",
    "    df = df.with_columns(\n",
    "        pl.col(\"time_from_start\").round(0).alias(\"time_rounded\")\n",
    "    )\n",
    "    \n",
    "    # Extract the metrics using the rounded time\n",
    "    vu_counts = df.filter(pl.col(\"metric\") == \"vus\")\n",
    "    input_tokens = df.filter(pl.col(\"metric\") == \"inputTokens\")\n",
    "    output_tokens = df.filter(pl.col(\"metric\") == \"outputTokens\")\n",
    "    \n",
    "    # Sort all dataframes by rounded time\n",
    "    vu_counts = vu_counts.sort(\"time_rounded\")\n",
    "    input_tokens = input_tokens.sort(\"time_rounded\")\n",
    "    output_tokens = output_tokens.sort(\"time_rounded\")\n",
    "    \n",
    "    # Create a combined dataframe with all unique rounded time points\n",
    "    all_times = pl.concat([\n",
    "        vu_counts.select(\"time_rounded\"),\n",
    "        input_tokens.select(\"time_rounded\"),\n",
    "        output_tokens.select(\"time_rounded\")\n",
    "    ]).unique().sort(\"time_rounded\")\n",
    "    \n",
    "    # For each time point, we'll calculate the tokens in the past window_size seconds\n",
    "    result_rows = []\n",
    "    \n",
    "    for time_point in all_times[\"time_rounded\"]:\n",
    "        window_start = time_point - window_size\n",
    "        \n",
    "        # Find the latest VU count at or before this time\n",
    "        vu_at_time = vu_counts.filter(pl.col(\"time_rounded\") <= time_point)\n",
    "        if len(vu_at_time) > 0:\n",
    "            latest_vu = vu_at_time.sort(\"time_rounded\", descending=True).head(1)[\"value\"][0]\n",
    "            \n",
    "            # Skip if VU count is below minimum threshold\n",
    "            if latest_vu < min_vu:\n",
    "                continue\n",
    "        else:\n",
    "            continue  # Skip if no VU data available\n",
    "        \n",
    "        # Calculate input tokens in the window\n",
    "        input_in_window = input_tokens.filter(\n",
    "            (pl.col(\"time_rounded\") > window_start) & \n",
    "            (pl.col(\"time_rounded\") <= time_point)\n",
    "        )\n",
    "        input_token_count = input_in_window[\"value\"].sum() if len(input_in_window) > 0 else 0\n",
    "        \n",
    "        # Calculate output tokens in the window\n",
    "        output_in_window = output_tokens.filter(\n",
    "            (pl.col(\"time_rounded\") > window_start) & \n",
    "            (pl.col(\"time_rounded\") <= time_point)\n",
    "        )\n",
    "        output_token_count = output_in_window[\"value\"].sum() if len(output_in_window) > 0 else 0\n",
    "        \n",
    "        # Calculate tokens per second\n",
    "        input_tokens_per_second = input_token_count / window_size\n",
    "        output_tokens_per_second = output_token_count / window_size\n",
    "        \n",
    "        result_rows.append({\n",
    "            \"time_from_start\": time_point,\n",
    "            \"vu_count\": latest_vu,\n",
    "            \"input_tokens_per_second\": input_tokens_per_second,\n",
    "            \"output_tokens_per_second\": output_tokens_per_second,\n",
    "            \"total_tokens_per_second\": input_tokens_per_second + output_tokens_per_second\n",
    "        })\n",
    "    \n",
    "    # Convert to Polars DataFrame\n",
    "    rolling_df = pl.DataFrame(result_rows)\n",
    "    \n",
    "    # Group by VU count to get averages\n",
    "    if len(rolling_df) > 0:\n",
    "        tokens_by_vu = rolling_df.group_by(\"vu_count\").agg(\n",
    "            pl.col(\"input_tokens_per_second\").mean().alias(\"avg_input_tokens_per_second\"),\n",
    "            pl.col(\"output_tokens_per_second\").mean().alias(\"avg_output_tokens_per_second\"),\n",
    "            pl.col(\"total_tokens_per_second\").mean().alias(\"avg_total_tokens_per_second\"),\n",
    "            pl.count().alias(\"sample_count\")\n",
    "        ).sort(\"vu_count\")\n",
    "        \n",
    "        # Additional filter to ensure we only include VU counts >= min_vu\n",
    "        tokens_by_vu = tokens_by_vu.filter(pl.col(\"vu_count\") >= min_vu)\n",
    "        \n",
    "        return tokens_by_vu\n",
    "    else:\n",
    "        # Return empty DataFrame with correct schema if no data\n",
    "        return pl.DataFrame({\n",
    "            \"vu_count\": [],\n",
    "            \"avg_input_tokens_per_second\": [],\n",
    "            \"avg_output_tokens_per_second\": [],\n",
    "            \"avg_total_tokens_per_second\": [],\n",
    "            \"sample_count\": []\n",
    "        })\n",
    "\n",
    "\n",
    "def get_gpt4o_mini_price(df):\n",
    "    total_input_tokens = df.filter(pl.col(\"metric\") == \"inputTokens\").select(\n",
    "        pl.col(\"value\")).sum().item()\n",
    "    total_output_tokens = df.filter(pl.col(\"metric\") == \"outputTokens\").select(\n",
    "        pl.col(\"value\")).sum().item()\n",
    "    gpt4o_mini_price = gpt4o_mini_price_per_million[\"input\"] * total_input_tokens * \\\n",
    "        gpt4o_mini_token_multiplier / 1e6 + \\\n",
    "        gpt4o_mini_price_per_million[\"output\"] * total_output_tokens / 1e6\n",
    "    return gpt4o_mini_price\n",
    "\n",
    "\n",
    "def find_best_throughput(tokens_by_vu_df):\n",
    "    \"\"\"\n",
    "    Finds the best total token throughput and the corresponding VU count.\n",
    "    \n",
    "    Parameters:\n",
    "    tokens_by_vu_df (pl.DataFrame): Output from tokens_per_second_by_vu function\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (best_throughput, optimal_vu_count, throughput_data)\n",
    "        - best_throughput (float): Maximum total tokens per second\n",
    "        - optimal_vu_count (float): VU count that achieved the maximum throughput\n",
    "        - throughput_data (dict): Dictionary with detailed throughput metrics\n",
    "    \"\"\"\n",
    "    if len(tokens_by_vu_df) == 0:\n",
    "        return (0, 0, {\n",
    "            \"best_throughput\": 0,\n",
    "            \"optimal_vu_count\": 0,\n",
    "            \"input_throughput\": 0,\n",
    "            \"output_throughput\": 0,\n",
    "            \"sample_count\": 0\n",
    "        })\n",
    "    \n",
    "    # Sort by total throughput in descending order and get the top row\n",
    "    best_row = tokens_by_vu_df.sort(\"avg_total_tokens_per_second\", descending=True).head(1)\n",
    "    \n",
    "    # Extract the values\n",
    "    best_throughput = best_row[\"avg_total_tokens_per_second\"][0]\n",
    "    optimal_vu_count = best_row[\"vu_count\"][0]\n",
    "    input_throughput = best_row[\"avg_input_tokens_per_second\"][0]\n",
    "    output_throughput = best_row[\"avg_output_tokens_per_second\"][0]\n",
    "    sample_count = best_row[\"sample_count\"][0]\n",
    "    \n",
    "    # Create a dictionary with detailed metrics\n",
    "    throughput_data = {\n",
    "        \"best_total_throughput\": best_throughput,\n",
    "        \"optimal_vu_count\": optimal_vu_count,\n",
    "        \"input_throughput\": input_throughput,\n",
    "        \"output_throughput\": output_throughput,\n",
    "        \"sample_count\": sample_count\n",
    "    }\n",
    "    \n",
    "    return throughput_data\n",
    "\n",
    "\n",
    "def calculate_token_prices(throughput_data, cost_of_cluster_per_second, output_multiplier=4):\n",
    "    \"\"\"\n",
    "    Calculate the price per token given cluster cost and throughput data, with output tokens costing \n",
    "    more than input tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    throughput_data (dict): Output from find_best_throughput function\n",
    "    cost_of_cluster_per_second (float): Cost of running the cluster per second in dollars\n",
    "    output_multiplier (float): How much more output tokens cost vs input (default: 4)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (input_price_per_token, output_price_per_token, price_details)\n",
    "    \"\"\"\n",
    "    # Extract the values from throughput_data\n",
    "    input_tokens_per_second = throughput_data[\"input_throughput\"]\n",
    "    output_tokens_per_second = throughput_data[\"output_throughput\"]\n",
    "    \n",
    "    # Calculate token prices using constraint: cost = input_price*input_tokens + output_price*output_tokens\n",
    "    # where output_price = input_price * output_multiplier\n",
    "    \n",
    "    weighted_token_sum = input_tokens_per_second + (output_multiplier * output_tokens_per_second)\n",
    "    \n",
    "    if weighted_token_sum > 0:\n",
    "        input_price_per_token = cost_of_cluster_per_second / weighted_token_sum\n",
    "        output_price_per_token = input_price_per_token * output_multiplier\n",
    "        \n",
    "        # Calculate price per million tokens\n",
    "        input_price_per_million = input_price_per_token * 1_000_000\n",
    "        output_price_per_million = output_price_per_token * 1_000_000\n",
    "        \n",
    "        # Calculate the cost allocation\n",
    "        input_cost = input_tokens_per_second * input_price_per_token\n",
    "        output_cost = output_tokens_per_second * output_price_per_token\n",
    "        \n",
    "        # Calculate percentage of cluster cost allocated to each token type\n",
    "        input_cost_percentage = (input_cost / cost_of_cluster_per_second) * 100\n",
    "        output_cost_percentage = (output_cost / cost_of_cluster_per_second) * 100\n",
    "        \n",
    "        price_details = {\n",
    "            \"input_price_per_token\": input_price_per_token,\n",
    "            \"output_price_per_token\": output_price_per_token,\n",
    "            \"input_price_per_million\": input_price_per_million,\n",
    "            \"output_price_per_million\": output_price_per_million,\n",
    "            \"input_cost_percentage\": input_cost_percentage,\n",
    "            \"output_cost_percentage\": output_cost_percentage,\n",
    "            \"input_tokens_per_second\": input_tokens_per_second,\n",
    "            \"output_tokens_per_second\": output_tokens_per_second,\n",
    "            \"total_tokens_per_second\": input_tokens_per_second + output_tokens_per_second\n",
    "        }\n",
    "        \n",
    "        return (input_price_per_token, output_price_per_token, price_details)\n",
    "    else:\n",
    "        # Handle the case where there are no tokens\n",
    "        return (0, 0, {\n",
    "            \"error\": \"No token throughput data available\"\n",
    "        })\n",
    "\n",
    "\n",
    "def process_gpu(gpu):\n",
    "    df, cost = get_df(gpu)\n",
    "    tokens_by_vu = tokens_per_second_by_vu(df, window_size=60.0)\n",
    "    throughput_data = find_best_throughput(tokens_by_vu)\n",
    "    print(throughput_data)\n",
    "    # gpt4o_mini_price = get_gpt4o_mini_price(df)\n",
    "    # avg_input_tokens_per_image = df.filter(pl.col(\"metric\") == \"inputTokens\").select(pl.col(\"value\")).mean().item()\n",
    "    # test_duration = df.filter(pl.col(\"metric\") == \"http_req_duration\").select(pl.col(\"time_from_start\")).max().item()\n",
    "    max_nodes = df.filter(pl.col(\"metric\") == \"node_count\").select(pl.col(\"value\")).max().item()\n",
    "    cost_of_cluster_per_second = max_nodes * cost[\"batch\"] / 3600\n",
    "    input_price_per_token, output_price_per_token, price_details = calculate_token_prices(\n",
    "        throughput_data, cost_of_cluster_per_second)\n",
    "    print(price_details)\n",
    "    \n",
    "\n",
    "\n",
    "gpus = [\"3090\"]\n",
    "for gpu in gpus:\n",
    "    print(json.dumps(process_gpu(gpu), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
