Run large language models with [vLLM](https://docs.vllm.ai/), a high-performance inference engine built for fast and efficient model serving. vLLM implements [PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html) for optimized memory use and supports the [OpenAI Messages API](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html), streaming responses, JSON mode, and more.

<Callout variation="warning">Ensure your use is permissible under whatever license applies to the model you are using.</Callout>

Models are **not bundled with the container** and will be downloaded at runtime.  
**Startup may take several minutes**, especially for larger checkpoints.
For production use, we recommend prebuilding your model into the image.

---

## Running on Consumer GPUs

This recipe is designed for modern **consumer GPUs** (default: RTX 4090 24GB). These cards can comfortably handle models up to ~8B parameters, depending on precision and memory utilization settings.  

### Key Configuration Options

When deploying, you can customize the container using these options:

- **Model** — Required. Hugging Face model ID to load (default: `Qwen/Qwen2.5-7B-Instruct`).  
- **Hugging Face Token** — Your HF access token (only required for gated/private models).  
- **GPU Memory Utilization** — Fraction of GPU VRAM vLLM can use (default: `1`). Adjust if you need to leave memory headroom for monitoring or other processes.
- **Max Model Length** — Model context length (default: `4096`). 

<Callout variation="info">
Defaults are tuned for running 7B–8B models on a single RTX 4090. You can override them in *Advanced Configuration*.
</Callout>

---

### Example Supported Models

- **Qwen2.5 1.5B Instruct** — `Qwen/Qwen2.5-1.5B-Instruct` (default)
- **Qwen2.5 7B Instruct** — `Qwen/Qwen2.5-7B-Instruct` (default)
- **Llama 3.1 8B Instruct** — `meta-llama/Meta-Llama-3.1-8B-Instruct`
- **DeepSeek R1 Distill Llama 8B** — `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`


