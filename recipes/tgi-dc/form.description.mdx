Run popular LLMs and VLMs with [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index), a production-ready inference server for large-scale text generation tasks. TGI supports [OpenAI Messages API](https://huggingface.co/docs/text-generation-inference/reference/api_reference), streaming output, JSON mode, and more.

<Callout variation="warning">Ensure your use is permissible under whatever license applies to the model you are using.</Callout>

Models are not bundled with the container and will be downloaded at runtime based on your configuration. **This means container startup may take several minutes**, especially for large models.

TGI has many [configuration options](https://huggingface.co/docs/text-generation-inference/reference/launcher) that can be set using environment variables in the *Advanced Configuration* section. TGI automatically detects hardware, but you should manually configure key options.
---

## Multi-GPU and Datacenter Support

On SaladCloud Secure, you can access **8-GPU datacenter nodes** to serve very large models, including **70B LLMs**, using TGI’s sharded inference mode. To run models across multiple GPUs, configure the following environment variables:

- `CUDA Visible Devices` — Defines which GPUs the container can access. Example: `0,1,2,3,4,5,6,7` exposes all 8 GPUs. **Required** for multi-GPU support.
- `Number of Shards` — Number of shards to split the model across. Must be equal to or less than the number of visible GPUs.

- [See supported models](https://huggingface.co/docs/text-generation-inference/supported_models)
- This recipe is defaulted to run on 8×L40S GPUs per node (48GB VRAM each). These nodes are ideal for serving large models with high throughput and parallelism.
