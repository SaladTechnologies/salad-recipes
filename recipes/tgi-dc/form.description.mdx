Run popular LLMs and VLMs with [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index), a production-ready inference server for large-scale text generation tasks. TGI supports [OpenAI Messages API](https://huggingface.co/docs/text-generation-inference/reference/api_reference), streaming output, JSON mode, and more.

<Callout variation="warning">Ensure your use is permissible under whatever license applies to the model you are using.</Callout>

Models are not bundled with the container and will be downloaded at runtime based on your configuration. **This means container startup may take several minutes**, especially for large models.

TGI has many [configuration options](https://huggingface.co/docs/text-generation-inference/reference/launcher) that can be set using environment variables in the *Advanced Configuration* section. TGI automatically detects hardware, but you should manually configure key options.
---

## Multi-GPU and Datacenter Support

On SaladCloud Secure, you can access **8-GPU datacenter nodes** to serve very large models, including **70B LLMs**, using TGI’s sharded inference mode. To run models across multiple GPUs, configure the following environment variables:

- `Model` — Hugging Face model to load.
- `Hugging Face Token` — Your Hugging Face access token (if the model is private).

This recipe is defaulted to run on **8×L40S GPUs per node (48GB VRAM each)**. These nodes are ideal for serving large models with high throughput and parallelism.

[See all supported models](https://huggingface.co/docs/text-generation-inference/supported_models)

### Example Supported Models

- **Llama 2 70B** — `meta-llama/Llama-2-70b-chat-hf`  
- **Llama 3 70B** — `meta-llama/Meta-Llama-3-70B-Instruct`  
- **Mixtral 8x7B** — `mistralai/Mixtral-8x7B-Instruct-v0.1`  
- **DeepSeek R1 Distill Llama 70B** — `deepseek-ai/DeepSeek-R1-Distill-Llama-70B` (default)  